1)HDFS:
      Hadoop File System was developed using distributed file system design 
     It is run on commodity hardware. Unlike other distributed systems
     HDFS is highly faulttolerant and designed using low-cost hardware
     HDFS holds very large amount of data and provides easier access
     To store such huge data, the files are stored across multiple machines 
     These files are stored in redundant fashion to rescue the system from possible data losses in case of failure 
     HDFS also makes applications available to parallel processing
     It is suitable for the distributed storage and processing
     It has two types of node
     
                               *name node                           
                               *data node 
      

      name node:The namenode is the commodity hardware that contains name node server
                It is a software that can be run on commodity hardware
		The system having the namenode acts as the master server 
		It Manages the file system namespace
 		It Regulates client’s access to files
		It also executes file system operations such as renaming, closing, and opening files and directories
      
      data node:The datanode is a commodity hardware having the data node server
		For every node in a cluster, there will be a datanode. These nodes manage the data storage of their system
		Datanodes perform read-write operations on the file systems, as per client request
		They also perform operations such as block creation, deletion, and replication according to the instructions of the namenode.

===================================================================================================================================================


2)Hadoop cluster:
               A Hadoop cluster is a special type of computational cluster designed specifically for storing and analyzing huge amounts of unstructured data in a distributed computing environment
	       Such clusters run Hadoop's open source distributed processing software on low-cost commodity computers
               Typically one machine in the cluster is designated as the NameNode and another machine the as JobTracker these are the masters 
               The rest of the machines in the cluster act as both DataNode and TaskTracker these are the slaves 
               Hadoop clusters are often referred to as shared nothing systems because the only thing that is shared between nodes is the network that connects them
               Hadoop clusters are known for boosting the speed of data analysis applications
               They also are highly scalable 
               If a cluster's processing power is overwhelmed by growing volumes of data, additional cluster nodes can be added to increase throughput 
               Hadoop clusters also are highly resistant to failure because each piece of data is copied onto other cluster nodes which ensures that the data is not lost if one node fails
	       Large Hadoop Clusters are arranged in several racks
	       Network traffic between different nodes in the same rack is much more desirable than network traffic across the racks

========================================================================================================================================================================================================


3)HDFS blocks:
               Hadoop distributed file system also stores the data in terms of blocks
             However the block size in HDFS is very large
             The default size of HDFS block is 128MB
             The files are split into 128MB blocks and then stored into the hadoop filesystem 
             The hadoop application is responsible for distributing the data blocks across multiple nodes 
	     The blocks are of fixed size so it is very easy to calculate the number of blocks that can be stored on a disk
             HDFS block concept simplifies the storage of the datanodes
             The datanodes doesn’t need to concern about the blocks metadata data like file permissions 
             The namenode maintains the metadata of all the blocks
             If the size of the file is less than the HDFS block size, then the file does not occupy the complete block storage
             As the file is chunked into blocks, it is easy to store a file that is larger than the disk size as the data blocks are distributed and stored on multiple nodes in a hadoop cluster.
             Blocks are easy to replicate between the datanodes and thus provide fault tolerance and high availability
 	     Hadoop framework replicates each block across multiple nodes 
             The default replication factor is 3
             In case of any node failure or block corruption, the same block can be read from another node


